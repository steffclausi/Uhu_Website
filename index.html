<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ðŸ¦‰ Uhu-Ruf-Detektor (mit Worker)</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.20.0/dist/tf.min.js"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap');
        .custom-file-input::-webkit-file-upload-button {
            visibility: hidden;
        }
        .custom-file-input::before {
            content: 'Audiodateien auswÃ¤hlen';
            display: inline-block;
            background: #4f46e5;
            color: white;
            border-radius: 0.375rem;
            padding: 0.5rem 1rem;
            outline: none;
            white-space: nowrap;
            -webkit-user-select: none;
            cursor: pointer;
            font-weight: 500;
            margin-right: 1rem;
        }
        .custom-file-input:hover::before {
            background: #4338ca;
        }
        .custom-file-input:active::before {
            background: #3730a3;
        }
        .loader {
            border: 4px solid #f3f3f3;
            border-top: 4px solid #4f46e5;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <div class="container mx-auto p-4 md:p-8 max-w-7xl">
        <header class="text-center mb-8">
            <h1 class="text-4xl md:text-5xl font-bold text-gray-900">ðŸ¦‰ Uhu-Ruf-Detektor</h1>
            <p class="mt-4 text-lg text-gray-600">Laden Sie eine oder mehrere Audiodateien hoch, um sie automatisch nach Uhu-Rufen zu durchsuchen.</p>
        </header>

        <main class="bg-white p-6 md:p-8 rounded-2xl shadow-lg">
            
            <div class="mb-8 text-center">
                <label for="file-uploader" class="block text-lg font-medium text-gray-700 mb-2">Audiodateien hierher ziehen</label>
                <div class="mt-2 flex justify-center px-6 pt-5 pb-6 border-2 border-gray-300 border-dashed rounded-md">
                    <div class="space-y-1 text-center">
                        <svg class="mx-auto h-12 w-12 text-gray-400" stroke="currentColor" fill="none" viewBox="0 0 48 48" aria-hidden="true">
                            <path d="M28 8H12a4 4 0 00-4 4v20m32-12v8m0 0v8a4 4 0 01-4 4H12a4 4 0 01-4-4v-4m32-4l-3.172-3.172a4 4 0 00-5.656 0L28 28M8 32l9.172-9.172a4 4 0 015.656 0L28 28m0 0l4 4m4-24h8m-4-4v8" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
                        </svg>
                        <div class="flex text-sm text-gray-600">
                            <label for="file-uploader" class="relative cursor-pointer bg-white rounded-md font-medium text-indigo-600 hover:text-indigo-500 focus-within:outline-none focus-within:ring-2 focus-within:ring-offset-2 focus-within:ring-indigo-500">
                                <span>Dateien hochladen</span>
                                <input id="file-uploader" name="file-uploader" type="file" class="sr-only" multiple accept="audio/wav, audio/mp3, audio/flac, audio/m4a, audio/ogg, audio/opus">
                            </label>
                            <p class="pl-1">oder per Drag & Drop</p>
                        </div>
                        <p class="text-xs text-gray-500">WAV, MP3, FLAC, M4A, OGG, OPUS</p>
                    </div>
                </div>
                <div id="file-list" class="mt-4 text-left"></div>
            </div>

            <div id="parameters-section" class="hidden">
                <h2 class="text-2xl font-bold text-gray-800 border-b pb-2 mb-6">1. Parameter einstellen</h2>
                
                <div class="bg-gray-50 p-4 rounded-lg">
                    <details>
                        <summary class="font-medium cursor-pointer text-gray-700">Erweiterte Einstellungen</summary>
                        <div class="mt-4">
                            <div class="flex items-center mb-4">
                                <input id="merge-checkbox" type="checkbox" checked class="h-4 w-4 text-indigo-600 border-gray-300 rounded focus:ring-indigo-500">
                                <label for="merge-checkbox" class="ml-2 block text-sm text-gray-900">Ãœberlappende Erkennungen zusammenfÃ¼hren</label>
                            </div>
                            <div>
                                <label for="overlap-slider" class="block font-medium text-gray-700">Analyse-Ãœberlappung: <span id="overlap-value" class="font-bold text-indigo-600">50%</span></label>
                                <input id="overlap-slider" type="range" min="0" max="95" value="50" step="5" class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer">
                                <p class="text-sm text-gray-500 mt-1">Eine hÃ¶here Ãœberlappung findet kurze Rufe zuverlÃ¤ssiger, die Analyse dauert aber lÃ¤nger.</p>
                            </div>
                        </div>
                    </details>
                </div>
                
                <div class="mt-8 text-center">
                    <button id="start-analysis-btn" class="bg-indigo-600 text-white font-bold py-3 px-8 rounded-lg hover:bg-indigo-700 transition-colors duration-300 text-lg shadow-md">
                        ðŸš€ Analyse starten
                    </button>
                </div>
            </div>

            <div id="analysis-progress" class="hidden text-center my-8">
                 <div class="loader mx-auto"></div>
                 <p id="progress-text" class="mt-4 text-lg font-medium text-gray-600">Initialisiere Analyse...</p>
                 <div class="w-full bg-gray-200 rounded-full h-2.5 mt-2">
                     <div id="progress-bar" class="bg-indigo-600 h-2.5 rounded-full" style="width: 0%"></div>
                 </div>
            </div>

            <div id="results-section" class="hidden mt-10">
                <h2 id="results-title" class="text-2xl font-bold text-gray-800 border-b pb-2 mb-6">2. Ergebnisse</h2>
                <div id="results-summary"></div>
                <div id="results-grid" class="mt-6 grid grid-cols-1 sm:grid-cols-2 md:grid-cols-3 lg:grid-cols-4 gap-6"></div>
            </div>

        </main>
    </div>

    <script id="analysis-worker" type="javascript/worker">
        // Importiere TensorFlow.js im Worker-Scope
        self.importScripts('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.20.0/dist/tf.min.js');

        let model = null;
        let AUDIO_CONFIG = null; // Wird von der Hauptseite initialisiert

        // --- HILFSFUNKTIONEN (im Worker-Scope) ---

        function createMelFilterbank(numMelBins, numSpectrogramBins, sampleRate, lowerEdgeHz, upperEdgeHz) {
            const hzToMel = (hz) => 1127.0 * Math.log(1.0 + hz / 700.0);
            const melToHz = (mel) => 700.0 * (Math.exp(mel / 1127.0) - 1.0);
            const lowerMel = hzToMel(lowerEdgeHz);
            const upperMel = hzToMel(upperEdgeHz);
            const melPoints = tf.linspace(lowerMel, upperMel, numMelBins + 2).arraySync();
            const hzPoints = melPoints.map(melToHz);
            const spectrogramBinHz = sampleRate / 2.0 / (numSpectrogramBins - 1);
            const spectrogramBinEdges = Array.from({ length: numSpectrogramBins }, (_, i) => i * spectrogramBinHz);
            const melWeights = tf.buffer([numSpectrogramBins, numMelBins]);
            for (let i = 0; i < numMelBins; i++) {
                const leftEdge = hzPoints[i];
                const center = hzPoints[i + 1];
                const rightEdge = hzPoints[i + 2];
                for (let j = 0; j < numSpectrogramBins; j++) {
                    const specHz = spectrogramBinEdges[j];
                    let weight = 0.0;
                    if (specHz >= leftEdge && specHz <= center) {
                        const denominator = center - leftEdge;
                        if (denominator > 0) weight = (specHz - leftEdge) / denominator;
                    } else if (specHz > center && specHz <= rightEdge) {
                        const denominator = rightEdge - center;
                        if (denominator > 0) weight = (rightEdge - specHz) / denominator;
                    }
                    melWeights.set(weight, j, i);
                }
            }
            return melWeights.toTensor();
        }

        function audioToMelspectrogram(y) {
            if (y.length > AUDIO_CONFIG.TARGET_SAMPLES) {
                y = y.slice(0, AUDIO_CONFIG.TARGET_SAMPLES);
            } else if (y.length < AUDIO_CONFIG.TARGET_SAMPLES) {
                const padding = new Float32Array(AUDIO_CONFIG.TARGET_SAMPLES - y.length).fill(0);
                const newY = new Float32Array(AUDIO_CONFIG.TARGET_SAMPLES);
                newY.set(y);
                newY.set(padding, y.length);
                y = newY;
            }
            return tf.tidy(() => {
                const inputTensor = tf.tensor1d(y);
                const stft = tf.signal.stft(inputTensor, AUDIO_CONFIG.N_FFT, AUDIO_CONFIG.HOP_LENGTH);
                const freqBinCount = AUDIO_CONFIG.N_FFT / 2 + 1;
                const minBin = Math.round(AUDIO_CONFIG.MIN_FREQ * AUDIO_CONFIG.N_FFT / AUDIO_CONFIG.SAMPLE_RATE);
                const maxBin = Math.round(AUDIO_CONFIG.MAX_FREQ * AUDIO_CONFIG.N_FFT / AUDIO_CONFIG.SAMPLE_RATE);
                const bandpassMaskValues = new Float32Array(freqBinCount);
                for (let i = 0; i < freqBinCount; i++) {
                    if (i >= minBin && i <= maxBin) { bandpassMaskValues[i] = 1; } else { bandpassMaskValues[i] = 0; }
                }
                const bandpassMask = tf.tensor1d(bandpassMaskValues);
                const filteredStft = stft.mul(bandpassMask);
                const powerSpectrogram = tf.square(tf.abs(filteredStft));
                const melMatrix = createMelFilterbank(AUDIO_CONFIG.N_MELS, freqBinCount, AUDIO_CONFIG.SAMPLE_RATE, 0, AUDIO_CONFIG.SAMPLE_RATE / 2);
                const melSpectrogram = tf.matMul(powerSpectrogram, melMatrix);
                const logMelSpectrogram = tf.log(melSpectrogram.add(1e-6));
                return logMelSpectrogram.expandDims(-1);
            });
        }

        async function decodeAndStandardizeAudio(file) {
            const audioCtx = new OfflineAudioContext(1, 1, AUDIO_CONFIG.SAMPLE_RATE); // Dummy context to get target sample rate
            const arrayBuffer = await file.arrayBuffer();
            const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);
            if (audioBuffer.sampleRate === AUDIO_CONFIG.SAMPLE_RATE && audioBuffer.numberOfChannels === 1) {
                return audioBuffer.getChannelData(0);
            }
            const offlineCtx = new OfflineAudioContext(1, audioBuffer.duration * AUDIO_CONFIG.SAMPLE_RATE, AUDIO_CONFIG.SAMPLE_RATE);
            const source = offlineCtx.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(offlineCtx.destination);
            source.start(0);
            const renderedBuffer = await offlineCtx.startRendering();
            return renderedBuffer.getChannelData(0);
        }
        
        function mergeEvents(candidates) {
            if (!candidates || candidates.length === 0) return [];
            candidates.sort((a, b) => a.start - b.start);
            const merged = [Object.assign({}, candidates[0])];
            for (let i = 1; i < candidates.length; i++) {
                const last = merged[merged.length - 1];
                const current = candidates[i];
                if (current.start <= last.end) {
                    last.end = Math.max(last.end, current.end);
                    last.prob = Math.max(last.prob, current.prob);
                } else {
                    merged.push(Object.assign({}, current));
                }
            }
            return merged;
        }

        // --- WORKER EVENT LISTENER ---
        self.onmessage = async (e) => {
            const { file, params } = e.data;
            AUDIO_CONFIG = params.audioConfig; // Konfiguration vom Haupt-Thread Ã¼bernehmen

            try {
                // Lade Modell, falls noch nicht geschehen
                if (!model) {
                    self.postMessage({ type: 'progress', message: `Lade Erkennungsmodell...` });
                    model = await tf.loadLayersModel(params.modelPath);
                    // "Warm-up"
                    const specShape = model.inputs[0].shape.slice(1);
                    const dummyInput = tf.zeros([1, ...specShape]);
                    model.predict(dummyInput).dispose();
                    dummyInput.dispose();
                }

                self.postMessage({ type: 'progress', message: `Analysiere: ${file.name}` });

                const y = await decodeAndStandardizeAudio(file);
                const specs = [];
                const starts = [];
                const strideSec = AUDIO_CONFIG.DURATION * (1 - params.overlapPercent / 100.0);
                const strideSamples = Math.round(strideSec * AUDIO_CONFIG.SAMPLE_RATE);

                for (let start = 0; start + AUDIO_CONFIG.TARGET_SAMPLES <= y.length; start += strideSamples) {
                    const end = start + AUDIO_CONFIG.TARGET_SAMPLES;
                    let clip = y.slice(start, end);
                    const spec = audioToMelspectrogram(clip);
                    specs.push(spec);
                    starts.push(start);
                }

                let fileEvents = [];
                if (specs.length > 0) {
                    const X = tf.stack(specs);
                    const probs = await model.predict(X).data();
                    tf.dispose(X);
                    tf.dispose(specs);

                    let candidates = [];
                    for (let j = 0; j < probs.length; j++) {
                        if (probs[j] >= params.threshold) {
                            candidates.push({
                                start: starts[j],
                                end: starts[j] + AUDIO_CONFIG.TARGET_SAMPLES,
                                prob: probs[j],
                                file: file.name,
                                audioClip: y.slice(starts[j], starts[j] + AUDIO_CONFIG.TARGET_SAMPLES)
                            });
                        }
                    }
                    fileEvents = params.shouldMerge ? mergeEvents(candidates) : candidates;
                }
                
                // Sende Ergebnisse zurÃ¼ck zum Haupt-Thread
                self.postMessage({ type: 'result', events: fileEvents, fileName: file.name });

            } catch (error) {
                console.error(`Worker-Fehler bei der Verarbeitung von ${file.name}:`, error);
                self.postMessage({ type: 'error', message: `Fehler bei Datei '${file.name}': ${error.message}` });
            }
        };
    </script>
    
    <script type="module">
        // --- Globale Konfiguration ---
        const MODEL_PATH = './uhu_model_js/model.json';
        const MAX_CLIPS_TO_SHOW = 20;

        const AUDIO_CONFIG = {
            SAMPLE_RATE: 16000,
            DURATION: 3,
            N_MELS: 64,
            N_FFT: 1024,
            HOP_LENGTH: 512,
            TARGET_SAMPLES: 16000 * 3,
            MIN_FREQ: 200,
            MAX_FREQ: 5000,
        };

        // --- UI-Elemente ---
        const fileUploader = document.getElementById('file-uploader');
        const fileListDiv = document.getElementById('file-list');
        const paramsSection = document.getElementById('parameters-section');
        const overlapSlider = document.getElementById('overlap-slider');
        const overlapValue = document.getElementById('overlap-value');
        const mergeCheckbox = document.getElementById('merge-checkbox');
        const startBtn = document.getElementById('start-analysis-btn');
        const analysisProgress = document.getElementById('analysis-progress');
        const progressText = document.getElementById('progress-text');
        const progressBar = document.getElementById('progress-bar');
        const resultsSection = document.getElementById('results-section');
        const resultsTitle = document.getElementById('results-title');
        const resultsSummary = document.getElementById('results-summary');
        const resultsGrid = document.getElementById('results-grid');

        let uploadedFiles = [];

        // --- Event Listeners ---
        fileUploader.addEventListener('change', (e) => handleFiles(e.target.files));
        const dropZone = fileUploader.closest('.border-dashed');
        dropZone.addEventListener('dragover', (e) => {
            e.preventDefault(); e.stopPropagation();
            dropZone.classList.add('border-indigo-500', 'bg-indigo-50');
        });
        dropZone.addEventListener('dragleave', (e) => {
            e.preventDefault(); e.stopPropagation();
            dropZone.classList.remove('border-indigo-500', 'bg-indigo-50');
        });
        dropZone.addEventListener('drop', (e) => {
            e.preventDefault(); e.stopPropagation();
            dropZone.classList.remove('border-indigo-500', 'bg-indigo-50');
            handleFiles(e.dataTransfer.files);
        });

        overlapSlider.addEventListener('input', (e) => {
            overlapValue.textContent = `${e.target.value}%`;
        });
        startBtn.addEventListener('click', runAnalysis);

        function handleFiles(files) {
            uploadedFiles = Array.from(files);
            if (uploadedFiles.length > 0) {
                fileListDiv.innerHTML = `<h3 class="font-medium text-gray-700">AusgewÃ¤hlte Dateien:</h3><ul class="list-disc list-inside text-sm text-gray-600">${uploadedFiles.map(f => `<li>${f.name}</li>`).join('')}</ul>`;
                paramsSection.classList.remove('hidden');
                resultsSection.classList.add('hidden'); // Ergebnisse bei neuer Auswahl ausblenden
            } else {
                fileListDiv.innerHTML = '';
                paramsSection.classList.add('hidden');
            }
        }

        async function runAnalysis() {
            if (uploadedFiles.length === 0) {
                alert("Bitte wÃ¤hlen Sie zuerst Audiodateien aus.");
                return;
            }

            // UI fÃ¼r Analyse vorbereiten
            startBtn.disabled = true;
            startBtn.classList.add('opacity-50', 'cursor-not-allowed');
            analysisProgress.classList.remove('hidden');
            resultsSection.classList.add('hidden');
            resultsGrid.innerHTML = '';
            resultsSummary.innerHTML = '';
            progressText.textContent = 'Initialisiere Analyse...';
            progressBar.style.width = '0%';

            const allEvents = [];
            let filesProcessed = 0;

            // Worker-Blob erstellen
            const workerScript = document.getElementById('analysis-worker').textContent;
            const workerBlob = new Blob([workerScript], { type: 'application/javascript' });
            const workerUrl = URL.createObjectURL(workerBlob);
            
            // Worker erstellen
            const worker = new Worker(workerUrl);

            // Handler fÃ¼r Nachrichten vom Worker
            worker.onmessage = (e) => {
                const { type, message, events, error } = e.data;

                if (type === 'progress') {
                    progressText.textContent = message;
                } else if (type === 'result') {
                    allEvents.push(...events);
                    filesProcessed++;
                    progressText.textContent = `Analyse abgeschlossen fÃ¼r: ${e.data.fileName}`;
                    progressBar.style.width = `${(filesProcessed / uploadedFiles.length) * 100}%`;

                    // NÃ¤chste Datei senden oder Analyse beenden
                    if (uploadedFiles.length > 0) {
                        const nextFile = uploadedFiles.shift();
                        sendToWorker(nextFile);
                    } else {
                        worker.terminate();
                        finalizeAnalysis();
                    }
                } else if (type === 'error') {
                    console.error("Worker Error:", message);
                    filesProcessed++; // ZÃ¤hlen, um die Schleife zu beenden
                    if (uploadedFiles.length > 0) {
                         const nextFile = uploadedFiles.shift();
                         sendToWorker(nextFile);
                    } else {
                        worker.terminate();
                        finalizeAnalysis();
                    }
                }
            };

            worker.onerror = (e) => {
                console.error(`Ein Fehler ist im Worker aufgetreten: ${e.message}`);
                progressText.textContent = `Ein kritischer Fehler ist aufgetreten: ${e.message}`;
                finalizeAnalysis(true); // Analyse mit Fehler beenden
                worker.terminate();
            };

            const sendToWorker = (file) => {
                const params = {
                    modelPath: MODEL_PATH,
                    audioConfig: AUDIO_CONFIG,
                    overlapPercent: parseInt(overlapSlider.value),
                    shouldMerge: mergeCheckbox.checked,
                    threshold: 0.5
                };
                worker.postMessage({ file, params });
            };

            const finalizeAnalysis = (hadError = false) => {
                 if (!hadError) {
                    progressText.textContent = 'Analyse fÃ¼r alle Dateien abgeschlossen!';
                    displayResults(allEvents);
                 }
                
                 startBtn.disabled = false;
                 startBtn.classList.remove('opacity-50', 'cursor-not-allowed');
                 setTimeout(() => {
                     analysisProgress.classList.add('hidden');
                     progressBar.style.width = '0%';
                 }, 2000);
            };

            // Erste Datei zur Verarbeitung an den Worker senden
            const firstFile = uploadedFiles.shift();
            sendToWorker(firstFile);
        }

        // --- Ergebnisdarstellung (unverÃ¤ndert) ---

        function createWavBlob(samples, sampleRate) {
            const buffer = new ArrayBuffer(44 + samples.length * 2);
            const view = new DataView(buffer);

            const writeString = (view, offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };

            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + samples.length * 2, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true); // Mono
            view.setUint16(22, 1, true); // 1 Channel
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true); // Byte Rate
            view.setUint16(32, 2, true); // Block Align
            view.setUint16(34, 16, true); // Bits per Sample
            writeString(view, 36, 'data');
            view.setUint32(40, samples.length * 2, true);

            let offset = 44;
            for (let i = 0; i < samples.length; i++, offset += 2) {
                const s = Math.max(-1, Math.min(1, samples[i]));
                view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
            }

            return new Blob([view], { type: 'audio/wav' });
        }
        
        // TemporÃ¤re Funktion zum Zeichnen des Spektrogramms, da die Originalfunktion nicht mehr direkt auf die Verarbeitung zugreifen kann
        function drawSpectrogramFromAudio(audioClip, canvas) {
             const specTensorForViz = audioToMelspectrogramForViz(audioClip);
             drawSpectrogram(specTensorForViz, canvas);
             tf.dispose(specTensorForViz);
        }

        // Diese Funktion wird nur noch fÃ¼r die Visualisierung benÃ¶tigt und kann im Hauptthread bleiben.
        function audioToMelspectrogramForViz(y) {
             // Diese Funktion ist eine Kopie der Worker-Funktion, aber verwendet die globale AUDIO_CONFIG
            if (y.length > AUDIO_CONFIG.TARGET_SAMPLES) {
                y = y.slice(0, AUDIO_CONFIG.TARGET_SAMPLES);
            } else if (y.length < AUDIO_CONFIG.TARGET_SAMPLES) {
                const padding = new Float32Array(AUDIO_CONFIG.TARGET_SAMPLES - y.length).fill(0);
                const newY = new Float32Array(AUDIO_CONFIG.TARGET_SAMPLES);
                newY.set(y); newY.set(padding, y.length);
                y = newY;
            }
             return tf.tidy(() => {
                const stft = tf.signal.stft(tf.tensor1d(y), AUDIO_CONFIG.N_FFT, AUDIO_CONFIG.HOP_LENGTH);
                const powerSpectrogram = tf.square(tf.abs(stft));
                const melMatrix = createMelFilterbankForViz(AUDIO_CONFIG.N_MELS, AUDIO_CONFIG.N_FFT / 2 + 1, AUDIO_CONFIG.SAMPLE_RATE, 0, AUDIO_CONFIG.SAMPLE_RATE / 2);
                const melSpectrogram = tf.matMul(powerSpectrogram, melMatrix);
                const logMelSpectrogram = tf.log(melSpectrogram.add(1e-6));
                return logMelSpectrogram.expandDims(-1);
             });
        }
        
        function createMelFilterbankForViz(numMelBins, numSpectrogramBins, sampleRate, lowerEdgeHz, upperEdgeHz) {
            // Kopie der Worker-Funktion
            const hzToMel = (hz) => 1127.0 * Math.log(1.0 + hz / 700.0);
            const melToHz = (mel) => 700.0 * (Math.exp(mel / 1127.0) - 1.0);
            const lowerMel = hzToMel(lowerEdgeHz);
            const upperMel = hzToMel(upperEdgeHz);
            const melPoints = tf.linspace(lowerMel, upperMel, numMelBins + 2).arraySync();
            const hzPoints = melPoints.map(melToHz);
            const spectrogramBinHz = sampleRate / 2.0 / (numSpectrogramBins - 1);
            const spectrogramBinEdges = Array.from({ length: numSpectrogramBins }, (_, i) => i * spectrogramBinHz);
            const melWeights = tf.buffer([numSpectrogramBins, numMelBins]);
            for (let i = 0; i < numMelBins; i++) {
                const leftEdge = hzPoints[i]; const center = hzPoints[i + 1]; const rightEdge = hzPoints[i + 2];
                for (let j = 0; j < numSpectrogramBins; j++) {
                    const specHz = spectrogramBinEdges[j];
                    let weight = 0.0;
                    if (specHz >= leftEdge && specHz <= center) {
                        const d = center - leftEdge; if (d > 0) weight = (specHz - leftEdge) / d;
                    } else if (specHz > center && specHz <= rightEdge) {
                        const d = rightEdge - center; if (d > 0) weight = (rightEdge - specHz) / d;
                    }
                    melWeights.set(weight, j, i);
                }
            }
            return melWeights.toTensor();
        }

        function displayResults(events) {
            resultsSection.classList.remove('hidden');
            resultsTitle.textContent = `2. Ergebnisse: ${events.length} Uhu-Rufe gefunden`;

            if (events.length === 0) {
                resultsSummary.innerHTML = `<div class="bg-blue-100 border-l-4 border-blue-500 text-blue-700 p-4 rounded-md" role="alert"><p>Es wurden keine Ereignisse Ã¼ber dem eingestellten Schwellenwert gefunden.</p></div>`;
                return;
            }

            events.sort((a, b) => b.prob - a.prob);

            const csvContent = "data:text/csv;charset=utf-8," 
                + "file,start_s,end_s,probability\n" 
                + events.map(e => `${e.file},${(e.start / AUDIO_CONFIG.SAMPLE_RATE).toFixed(2)},${(e.end / AUDIO_CONFIG.SAMPLE_RATE).toFixed(2)},${e.prob.toFixed(3)}`).join("\n");
            
            const encodedUri = encodeURI(csvContent);
            resultsSummary.innerHTML = `<a href="${encodedUri}" download="uhu_events.csv" class="inline-block bg-green-600 text-white font-bold py-2 px-4 rounded-lg hover:bg-green-700 transition-colors">ðŸ“„ Alle ${events.length} Erkennungen als CSV herunterladen</a>`;

            const clipsToShow = events.slice(0, MAX_CLIPS_TO_SHOW);
            resultsGrid.innerHTML = `<h3 class="col-span-full text-xl font-bold text-gray-800 mt-6 mb-2">Erkannte Clips (Top ${clipsToShow.length})</h3>`;
            
            clipsToShow.forEach(event => {
                const card = document.createElement('div');
                card.className = 'bg-gray-50 p-4 rounded-lg shadow';
                
                const specCanvas = document.createElement('canvas');
                specCanvas.width = 200;
                specCanvas.height = 128;
                
                const audio = new Audio();
                const blob = createWavBlob(event.audioClip, AUDIO_CONFIG.SAMPLE_RATE);
                audio.src = URL.createObjectURL(blob);
                audio.controls = true;
                audio.className = 'w-full mt-2';

                card.innerHTML = `
                    <p class="text-sm font-medium truncate" title="${event.file}">${event.file}</p>
                    <p class="text-xs text-indigo-600 font-bold">Wahrscheinlichkeit: ${(event.prob * 100).toFixed(1)}%</p>
                `;
                card.appendChild(specCanvas);
                card.appendChild(audio);
                
                resultsGrid.appendChild(card);
                
                // Erstelle Spektrogramm nur fÃ¼r die Visualisierung
                drawSpectrogramFromAudio(event.audioClip, specCanvas);
            });
        }

        function drawSpectrogram(spec3D, canvas) {
            tf.tidy(() => {
                const spec2D = spec3D.squeeze(); // Entferne die Kanal-Dimension
                const specTransposed = spec2D.transpose(); // Transponieren fÃ¼r die Darstellung [mels, time]

                const ctx = canvas.getContext('2d');
                const [height, width] = specTransposed.shape; // HÃ¶he = N_MELS, Breite = Zeit
                const canvasWidth = canvas.width;
                const canvasHeight = canvas.height;
                
                const specArray = specTransposed.arraySync();
                
                let minVal = Infinity, maxVal = -Infinity;
                for(let i=0; i<height; i++) {
                    for(let j=0; j<width; j++) {
                        if(specArray[i][j] < minVal) minVal = specArray[i][j];
                        if(specArray[i][j] > maxVal) maxVal = specArray[i][j];
                    }
                }
                const range = maxVal - minVal;

                const colWidth = canvasWidth / width;
                const rowHeight = canvasHeight / height;

                const viridis = [
                    [68, 1, 84], [72, 40, 120], [62, 74, 137], [49, 104, 142], [38, 130, 142],
                    [31, 158, 137], [53, 183, 121], [109, 205, 89], [180, 222, 44], [253, 231, 37]
                ];

                const getColor = (value) => {
                    const i = Math.min(Math.max(Math.floor(value * (viridis.length - 1)), 0), viridis.length - 2);
                    const t = value * (viridis.length - 1) - i;
                    const r = Math.floor(viridis[i][0] + t * (viridis[i+1][0] - viridis[i][0]));
                    const g = Math.floor(viridis[i][1] + t * (viridis[i+1][1] - viridis[i][1]));
                    const b = Math.floor(viridis[i][2] + t * (viridis[i+1][2] - viridis[i][2]));
                    return `rgb(${r},${g},${b})`;
                };
                
                ctx.clearRect(0, 0, canvasWidth, canvasHeight);

                for (let x = 0; x < width; x++) {
                    for (let y = 0; y < height; y++) {
                        const value = specArray[y][x];
                        const normalized = (value - minVal) / range;
                        ctx.fillStyle = getColor(normalized);
                        ctx.fillRect(x * colWidth, canvasHeight - (y + 1) * rowHeight, colWidth, rowHeight);
                    }
                }
            });
        }

    </script>
</body>
</html>
